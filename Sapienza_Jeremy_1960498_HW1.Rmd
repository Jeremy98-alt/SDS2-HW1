---
title: "Sapienza Jeremy 1960498 - HW1"
author: "Sapienza Jeremy 1960498"
date: "4/24/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(cache = TRUE)
```

## **Part I**
### ***Fully Bayesian conjugate analysis of Rome car accidents***

$$~$$

## Setup

$$~$$

We start to extract the car accident in Rome of the year 2016 contained in a dataframe named ***roma***. 

```{r include=FALSE, echo=FALSE}
library(latex2exp)
load("D:/UNIVERSITA'/MASTER DEGREE - La Sapienza/SDS - Statistical Methods in Data Science/Tardella#Part2/Homework 1/2021_homework_01.RData")
```

```{r}
mydata <- subset(roma,subset=sign_up_number==104)
y_obs <- mydata$car_accidents # extract the column car_accidents to do the main operations 
y_obs; print(paste("The lenght of these observation is: ", length(y_obs), sep=""))
```

$$~$$

Using the observed outcomes of the number of car accidents we do a fully Bayesian analysis using as a statistical model a conditionally i.i.d. Poisson distribution $Y_1, ..., Y_n|\theta \sim Poisson(\theta)$ with unknown $\theta$ parameter. In this case we know that the **average number** of hourly car accidents occuring in Rome during the day is **3.22**

$$~$$

## Describing the observed data

$$~$$

Considering the $y_{obs}$ described above, we want to see some interesting features, such as:

```{r}
print(paste("The mean of the observations is: ", round(mean(y_obs), 3), sep =""))
print(paste("The variance of the observations is: ", round(var(y_obs), 3), sep =""))
print(paste("The median of the observations is: ", median(y_obs), sep =""))
print(paste("The mode of the observations is: ", y_obs[which.max(tabulate(match(y_obs, unique(y_obs))))], sep=""))
```

$$~$$

The distribution of the data follows this behaviour:

```{r fig.align="center", echo=FALSE}
hist(y_obs, col="orchid2", main="Distribution of the data")
```

$$~$$

## The ingredients of the Bayesian model

$$~$$

As we know to make an updated version of the parameter of interest is important to set up the Bayesian framework as we can see below: 

$$
\pi(\theta|y_1, ..., y_n) = \frac{\pi(y_1, ..., y_n|\theta)\pi(\theta)}{\pi(y_1, ..., y_n)}
$$

Where: 

- $\pi(y_1, ..., y_n|\theta)$ is the likelihood function
- $\pi(\theta)$ is the prior distribution
- $\pi(\theta|y_1, ..., y_n)$ is the posterior distribution

All of these probabilities are based on the $\theta$ parameter, here this parameter is the the core of the Bayesian model, whose task is to find it in the best way. 

$$~$$

Now, we are illustrating with much details each probability

$$~$$

### Likelihood

$$~$$

The likelihood function measures the goodness of fit of a statistical model to a sample of data for given values of the unknown parameter (the number of parameters depends on the statistical model that we use in that moment).

In this case each observation is distributed with the Poisson distribution with $\theta = 3.22$, as we can see below. The joint distribution with n observations follows these steps based on theta parameter:

$$
Y_1, ..., Y_n|\theta \sim f(y_1, ..., y_n|\theta) = \prod_{i=1}^{f} (y_i|\theta) = \prod_{i=1}^{n} \frac{e^{-\theta}\theta^{y_i}}{y_i!} 
$$

In the next steps we consider the likelihood function respect the theta parameter $\propto e^{-n\theta}\theta^{\sum_{i=1}^{n} y_i}$. The reason is to understand if the shape of the likelihood function marries the prior shape distribution, for having the exact calculus we need also to consider the constants. 

Given the theta parameter, we see a first behaviour of our function, after the multiplication of our prior belief the result changes. 

We show some plots to see the different behaviour of the data once a particular theta parameter is given:

```{r fig.align="center", echo=FALSE}
par(mfrow=c(2,2))

theta <- 3.22
plot(y_obs, dpois(y_obs, lambda = theta), xlab="observation", ylab="likehood", main=TeX("$\\pi(y_1,...,y_n|\\theta) \\sim Poisson(\\theta = 3.22)$"), lwd=2)
points(y_obs, dpois(y_obs, lambda = theta), col = "orchid2", pch = 19)
grid()

theta <- 4.22
plot(y_obs, dpois(y_obs, lambda = theta), xlab="observation", ylab="likehood", main=TeX("$\\pi(y_1,...,y_n|\\theta) \\sim Poisson(\\theta = 4.22)$"), lwd=2)
points(y_obs, dpois(y_obs, lambda = theta), col = "orchid2", pch = 19)
grid()

theta <- 5.22
plot(y_obs, dpois(y_obs, lambda = theta), xlab="observation", ylab="likehood", main=TeX("$\\pi(y_1,...,y_n|\\theta) \\sim Poisson(\\theta = 5.22)$"), lwd=2)
points(y_obs, dpois(y_obs, lambda = theta), col = "orchid2", pch = 19)
grid()

theta <- 6.22
plot(y_obs, dpois(y_obs, lambda = theta), xlab="observation", ylab="likehood", main=TeX("$\\pi(y_1,...,y_n|\\theta) \\sim Poisson(\\theta = 6.22)$"), lwd=2)
points(y_obs, dpois(y_obs, lambda = theta), col = "orchid2", pch = 19)
grid()
```

$$~$$

### Prior distribution

$$~$$

Prior distribution is an interesting way to describe our prior information in order to apply this value to the likelihood function and produce the same shape for the posterior distribution. After that, we can know more about of which parameter is good to our data. We can use the conjugacy strategy avoiding integrating the marginal probability on theta, in order to make simple calculus. The conjugacy is a way to see if the shape of two distributions belong to the same family:

Definition of **Conjugate**: *A class $P$ of prior distributions for $\theta$ is called conjugate for a sampling model $p(y|\theta)$ if $p(\theta) \in P \Rightarrow p(\theta|y) \in P$*

In this case the suitable prior distribution that leads the posterior within the same family turns out to be the **Gamma Distribution** family with density:

$$
X \sim Gamma(r,s), \,\,\,\,\,\, f(x) = \frac{r^{s}x^{s-1}e^{-rx}}{\Gamma(s)} \mathbb{I}_{(0, +\infty)}(x)  \,\,\,\,\, r,s > 0
$$

where r is the **rate** and s is the **shape**, so we write the prior as follows:

$$
\pi(\theta) = \frac{r^{s}\theta^{s-1}e^{-r\theta}}{\Gamma(s)} \mathbb{I}_{(0, +\infty)} (\theta)
$$

Now, we are focussing on choosing the suitable prior parameters r and s, considering the equation on finding the expectation and variance value as follows:

$$
\begin{cases}
  \mathbb{E}[\theta] = \frac{s}{r} \\
  \mathbb{Var}[\theta] = \frac{s}{r^2}
\end{cases}
\rightarrow
\begin{cases}
  s = \mathbb{E}[\theta] \cdot r \\
  \mathbb{Var}[\theta] = \frac{\mathbb{E}[\theta] \cdot r}{r^2}
\end{cases}
\rightarrow
\begin{cases}
  s = \mathbb{E}[\theta] \cdot \frac{\mathbb{E}[\theta]}{\mathbb{Var}[\theta]} \\
  r = \frac{\mathbb{E}[\theta]}{\mathbb{Var}[\theta]}
\end{cases}
$$

In this case we prefer considering as our prior subjective belief of $s = \mathbb{E}[\theta] \cdot \frac{\mathbb{E}[\theta]}{\mathbb{Var}[\theta]} \approx 2.46$ (where $\mathbb{E}[\theta] = 3.22$ and the $\mathbb{Var}[\theta] = \mathbb{Var}(y_{obs}) \approx 4.21$) and $r = \frac{\mathbb{E}[\theta]}{\mathbb{Var}[\theta]} \approx 0.76$

```{r echo=FALSE}
# prior parameters
E_x <- 3.22
s_prior <- (E_x^2)/var(y_obs)
r_prior <- E_x/var(y_obs)

# updated parameters
r_post <- r_prior + length(y_obs)
s_post <- s_prior + sum(y_obs)
```

**Note**: is true that some prior subjective beliefs couldn't assign properly well e.g. to assign as the variance these y_obs that don't represents the whole data, but a portion. We will see that considering the conjugancy strategy the result obtained adjust all of our chooices! For having a good proper prior values is better i.e. to make some simulations to have better values.

$$~$$

### Posterior update

$$~$$

Now, after defining the main concepts of the prior and likelihood functions, let's move on the posterior update concept $\pi(\theta|y_1, ..., y_n)$, We apply as we can shown above the conjugancy analysis, in this way:

$$
\pi(\theta|y_1, ..., y_n) = \frac{\pi(y_1, ..., y_n|\theta)\pi(\theta)}{\pi(y_1, ..., y_n)} \propto \pi(y_1, ..., y_n|\theta)\pi(\theta) \\ =e^{-n\theta}\theta^{\sum_{i=1}^{n} y_i}e^{-r\theta}\theta^{s-1} = e^{-(n+r)\theta}\theta^{(s+\sum_{i=1}^{n} y_i)-1} 
$$

In this way we avoid the constants to see the shape of the posterior distribution. The posterior is also a Gamma distribution due to the theta parameter expressed and for the prior distribution, the parameters described are:

$$
\pi(\theta|y_1, ..., y_n) \sim Gamma\Big(r^* = n + r, \,\,s^* = s+ \sum_{i=1}^{n} y_i\Big)
$$

Now, the $\theta$ parameter is updated and fits well the data. Now, $r_{post} = 19.58$ and $s_{post} = 76.46$. 

$$~$$

# Report the main inferential findings

$$~$$

### Point estimates

$$~$$

All the inferential tasks will be driven by $\pi(\cdot|y^{obs})$ posterior distribution, such as in this case with the point estimates.

We can use the different point estimates to summarize the uncertainty (theta) distribution with only one value (using sometimes the closed forms provided by each distribution, in this case we refer to the closed forms of Gamma distribution). The most common summary statistics for posterior distributions are the posterior mean, posterior mode and posterior median.:

$$~$$

- **posterior mode** $\hat{\theta}_{bayes}^{mode}$: this represents the higher value of $\theta$ for which we want: $\hat{\theta}_{bayes}^{mo} = \operatorname{argmax}_{\theta \in \Theta} \pi(\theta|y^{obs}) = \frac{s^*-1}{r^*}$

```{r fig.align="center", echo=FALSE}

print(paste("The posterior mode is equal to: ", round((s_post-1)/r_post, 4), sep=""))
```

A mode is the most frequent point in a data set. In a Bayesian distribution, this refers to the peak of the distribution

$$~$$

- **posterior median** $\hat{\theta}_{bayes}^{med}$: this represents the $\theta$ value which divides the distribution in half for which: \ $$\hat{\theta}_{bayes}^{med} = {\displaystyle \int_{-\infty}^{\hat{\theta}_{bayes}^{med}}} \pi(\theta|y^{obs}) \,d\theta = {\displaystyle \int_{\hat{\theta}_{bayes}^{med}}}^{+\infty} \pi(\theta|y^{obs}) \,d\theta = \frac{1}{2}$$

```{r fig.align="center", echo=FALSE}
print(paste("The posterior median is equal to: ", round(qgamma(0.5, shape=s_post, rate=r_post), 4), sep=""))
```

We consider to find the $\theta$ value which has 50% of the probability to the left and 50% to the right.

$$~$$

- **posterior mean** $\hat{\theta}_{bayes}^{mean}$: this represents: $\hat{\theta}_{bayes}^{mean} =   \mathbb{E}(\theta|y^{obs}) = \frac{s^*}{r^*}$

```{r fig.align="center", echo=FALSE}
print(paste("The posterior mean is equal to: ", round(s_post/r_post, 4), sep=""))
```

The posterior mean is the expected value of the parameter estimate, using the posterior distribution.

$$~$$

Here we can see the point estimates over the posterior uncertainty:

```{r fig.align="center", echo=FALSE}
curve(dgamma(x, rate=r_post, shape=s_post), from=2.5, to=5, xlab=expression(theta), ylab= TeX("$\\pi(\\theta | y_1, ..., y_n)$"), main="", cex.main=0.5,col="red2")
title(main = "Point estimates", col.main= "black")
abline(v=3.8531, lty=1, col = "black")
abline(v=3.8872, lty=1, col = "green")
abline(v=3.9042,lty=1, col = "purple")
legend("topright", legend=c("Mode", "Median", "Mean"), col=c("black", "green", "purple"), lty=1, cex=0.8)
grid()
```

As we known these point estimations are used to measure the central tendency of the posterior distribution, mainly the exact distribution of the posterior is not known and for this reason is necessary to pick a measure that represents with one value the distribution. Typically the proper central tendency is basing on the distribution that we have. 

We prefer sometimes to use the posterior mode that is more precise and gives the maximum theta value of our posterior distribution. The median and the mean have typically the same behaviour. The choice of a point estimate could depend also on posterior loss, for a particular decision problem there is the relative point estimate to use, for example:

- if we have a loss function that is an absolute loss, the best estimate is the posterior median
- if we have a loss function that is a quadratic loss, the best estimate is the posterior mean
- if we have a loss function that is an "all-or-nothing" loss, the best estimate is the posterior mode

Basically, the **distribution** that we are analyzing is **symmetric** (not necessary should be exactly in the center) so the values treated before are similar for this reason.

Another interesting *hint* is that sometimes is true that the posterior mode doesn't give good result, basically when the distribution is irregular, but depends on the cases.

$$~$$

### Posterior uncertainty

$$~$$

The posterior uncertainty is expressed as we can seen in the last chapter as:

```{r}
r_post <- r_prior + length(y_obs)
s_post <- s_prior + sum(y_obs)

curve(dgamma(x, rate=r_post, shape=s_post), from=0, to=10, xlab=expression(theta), ylab= TeX("$\\pi(\\theta | y_1, ..., y_n)$"), main="", cex.main=0.5,col="red2")
title(main = "Posterior", col.main= "black")
grid()
```

This means that the good $\theta$ parameter tends to be in the interval between 3 and 5, but for this we will see more in details how to measure this uncertainty. The Posterior probability is a conditional probability conditioned on randomly observed data. Hence it is a random variable. For a random variable, it is important to summarize its amount of uncertainty. One way to achieve this goal is to provide a credible interval of the posterior probability.

$$~$$

### Interval Estimates

$$~$$

Often we prefer identifying regions of the parameter space that likely contains the true value of the parameter. To do this, after observing the data Y we construct a credible interval [l(y), u(y)] such that the probability that l(y) < $\theta$ < u(y) is large.

We prefer in this exercise to use the **HPD (Higher posterior density) region**. The HPD has the nice property that any point within the interval has a **higher density** than other point outside, so this interval is the collection of most likely values of the parameters.

The HPD is defined as the 100 x (1-$\alpha$%) of a subset of the parameter space, $s(y) \subset \Theta$ such that:

1. $\pi(\theta \in s(y)|Y=y) = 1 - \alpha$
2. if $\theta_{a} \in s(y)$ and $\theta_{b} \notin s(y)$, then $\pi(\theta_{a}|Y=y) > \pi(\theta_{b}|Y=y)$

As we can see below, there are the values and the range where the hypothetical $\theta$ could is

$$~$$

```{r fig.align="center", echo=FALSE, warning=FALSE}
library(TeachingDemos)

posterior_qf <- function(x) qgamma(x, shape = s_post, rate = r_post)
posterior_icdf <- function(x) qgamma(x, shape = s_post, rate = r_post)

interval_estimate_hpd <- TeachingDemos::hpd(posterior.icdf = posterior_qf, conf=0.95)

print("The HPD region in this case is between: ")
range(interval_estimate_hpd)

curve(dgamma(x, rate=r_post, shape=s_post), from=0, to=10, xlab=expression(theta), ylab= TeX("$\\pi(\\theta | y_1, ..., y_n)$"), main="", cex.main=0.5,col="red2")
title(main = "HPD Region", col.main= "black")
abline(v=interval_estimate_hpd[1],lty=3)
abline(v=interval_estimate_hpd[2],lty=3)
abline(h=dgamma(interval_estimate_hpd[1], shape = s_post, rate = r_post), lty=3)
grid()

```


$$~$$

### Difference between the prior and the posterior distribution

$$~$$

The prior distribution incorporates the subjective beliefs about parameters, since a new evidence is introduced. But, sometimes the prior distribution can be *uninformative* or *informative*, if we consider the beta uniform distribution the posterior will be driven entirely by the data (likelihood function). In this case we have an informative distribution in fact the prior shape tells more about the hyphotetical posterior shape. The posterior distribution should be a better way to describe the truth of a data generating process than the prior probability, since the posterior includes more information.

```{r fig.align="center", echo=FALSE}
par(mfrow=c(1, 2))

curve(dgamma(x, rate=r_prior, shape=s_prior), from=0, to=15, xlab=expression(theta), ylab= TeX("$\\pi(\\theta)$"), cex.main=0.5,col="blue2")
title(main = "Prior", col.main= "black")
grid()

curve(dgamma(x, rate=r_post, shape=s_post), from=0, to=15, xlab=expression(theta), ylab= TeX("$\\pi(\\theta | y_1, ..., y_n)$"), main="", cex.main=0.5,col="red2")
title(main = "Posterior", col.main= "black")
grid()
```

As we can see the posterior is slightly different on its direction, after the updated value of the $\theta$ parameter

```{r fig.align="center", echo=FALSE}
curve(dgamma(x, rate=r_prior, shape=s_prior), from=0, to=15, xlab=expression(theta), ylab= TeX("$\\pi(\\theta) + \\pi(\\theta | y_1, ..., y_n)$"), main="", cex.main=0.5,col="blue2", ylim= c(0, 1))
grid()

curve(dgamma(x, rate=r_post, shape=s_post), from=0, to=15, xlab=expression(theta), main="", cex.main=0.5,col="red2", add=TRUE, ylim= c(0, 1))
grid()

title(main = "Prior + Posterior", col.main= "black")
legend("topright", legend=c("Prior", "Posterior"), col=c("blue", "red"), lty=1, cex=0.8)
```


$$~$$

### (optional) Provide a formal definition of the posterior predictive distribution of $Y_{next}|y_1, ..., y_n$ and try to compare the posterior predictive distribution for a future observable with the actually observed data

$$~$$

Predictions about other more data are obtained with the posterior predictive distribution, in this subchapter we illustrate with formal steps the posterior predictive distribution for $Y_{next}|y_1, ..., y_n$

Suppose to have: 

$$
\begin{cases}
  Y_{next}, Y|\theta \sim f(y_{next}, y|\theta) = f(y_{next}|\theta) f(y|\theta) \\
  \theta \sim Gamma(r, s)
\end{cases}
$$

where $Y_{next}$ and $Y$ are conditional independent wrt $\theta$ and  $f(y_{next}, y|\theta) \sim Poisson(\theta)$
Given a observation $Y = y$ we find what is $Y_{next}$ conditioned on $y$, that means: 

$$
Y_{next}|y \sim \frac{J(y_{next}, y)}{J(y)} = m(y_{next}, y)
$$

this is called as (conditional) posterior predictive, we are interested to have:

$$
m(y_{next}|y) \propto J(y_{next}, y)
$$

Working on the joint, we have:

$$
J(y_{next}, y) = {\displaystyle \int_{\Theta} J(y_{next}, y, \theta) \,d\theta} \\
= {\displaystyle \int_{\Theta} J(y_{next}|\theta) J(y|\theta) \pi(\theta) \,d\theta} \\
\propto {\displaystyle \int_{\Theta} f(y_{next}|\theta) \frac{f(y|\theta)\pi(\theta)}{m(y)} \,d\theta} \\
= {\displaystyle \int_{\Theta} f(y_{next}|\theta) \pi(\theta|y) \,d\theta} \\
$$

for the density of Y we get:

$$
J(y) = \int_{\mathcal{Y}} \int_{\Theta} J(y_{next}, y, \theta) \,dy_{next}d\theta \\
= \int_{\mathcal{Y}} \int_{\Theta} f(y_{next}|\theta) f(y|\theta) \pi(\theta) \,dy_{next}d\theta \\
= \int_{\Theta}  f(y|\theta) \Bigg(\int_{\mathcal{Y}} f(y_{next}|\theta) \pi(\theta) \,dy_{next} \Bigg) d\theta \\
= \int_{\Theta}  f(y|\theta) \pi(\theta) d\theta = m(y)
$$

For predicting a new Gamma observation $Y_{next}$ after observing a $Y = y$ observation we can use the $m(y_{next}|y)$ and we have the **Negative Binomial Distribution** as follows:

$$
Y_{next}|y \sim NegBin \Bigg( p = \frac{r_{prior}+n}{r_{prior}+n+1}, m = s_{prior} + \sum_{i=1}^{n} y_i\Bigg)
$$

below is shown a simple code to illustrate this kind of posterior prediction based on the y_obs:

```{r}
n <- length(y_obs)
p <- (r_prior+n)/(r_prior+n+1)
m <- (s_prior+sum(y_obs))

plot(0:15, dnbinom(0:15,p = p, size = m),pch=6,col="red", main = TeX("Posterior Predictive Distribution $Y_{next}|y_1, ..., y_n \\sim NegBin (p, m)$"), xlab = TeX("$\\theta$"), ylab = TeX("$\\pi(Y_{next}|y_1, ..., y_n)$"))

post_mean <- (s_prior+sum(y_obs))/(r_prior+n)
points(0:15,dpois(0:15,lambda = post_mean),pch=6,col="blue")
grid()

legend("topright", legend=c("Predictions", "Observations"), col=c("red", "blue"), pch=6, cex=0.8)
```


$$~$$

## **Part II**
### ***Bulb lifetime***

$$~$$

## Setup

$$~$$

For this setting imagine there are innovative bulbs and the goal is to characterize them statistically. We consider 20 innovative bulbs to determine their lifetimes, I observe the follwing data expressed in terms of hours:

```{r fig.align="center", echo=FALSE}
y_obs_bulbs <- c(1, 13, 27, 43, 73, 75, 154, 196, 220, 297,
344, 610, 734, 783, 796, 845, 859, 992, 1066, 1471)
y_obs_bulbs
summary(y_obs_bulbs)
```

The distribution of these data is:

```{r fig.align="center", echo=FALSE}
hist(y_obs_bulbs, col="orchid2", main="Distribution of the data")
```

Based on the experience with light bulbs, These observations $Y_i$ described their lifetimes using an **exponential distribution** conditionally on $\theta$, the likelihood function is written in this way:

$$
f(y_1, ..., y_n|\theta) \sim Exponential(\theta)\\
Y_1, ..., Y_n|\theta \sim f(y_1, ..., y_n|\theta) = \prod_{i=1}^{f} (y_i|\theta) = \prod_{i=1}^{n} \theta e^{\theta  y_i} = \theta^{n}e^{-\theta \sum_{i=1}^{n} y_i}
$$

$$~$$

## The ingredients of the Bayesian model

$$~$$

As we described in the first part of this homework, we are interested to calculate the posterior distribution in order to update well the parameter of interest. Now, we are focussing on the choice of the suitable prior distribution.

For this exercise in order to choose a suitable prior distribution $\pi(\theta)$ we decide to pick the Gamma distribution to be able to apply the conjugate analysis:

$$
\pi(\theta) = \frac{r^{s}\theta^{s-1}e^{-r\theta}}{\Gamma(s)} \mathbb{I}_{(0, +\infty)} (\theta) \\
where \,\,\, \mathbb{E}[\theta] = 0.003 \,\,\, and \,\,\, \mathbb{V}[\theta] = (0.00173)^2 and \,\, r,s > 0
$$

```{r fig.align="center", echo=FALSE}
E_x <- 0.003
Var_x <- (0.00173)*(0.00173)

r_prior <- E_x/Var_x
s_prior <- E_x*r_prior

curve(dgamma(x, rate=r_prior, shape=s_prior), from=0, to=0.010, xlab=expression(theta), ylab= TeX("$\\pi(\\theta)$"), main="", cex.main=0.5,col="blue2", ylim= c(0, 900))
title(main = "Prior Distribution", col.main= "black")
grid()
```

$$~$$

The vague prior opinion is that the standard deviation is high, this means that the mean of the values is unstable the dispersion of our values is high. In this plot we see that the different is high.

<!-- ```{r echo=FALSE} -->
<!-- paste("the variance of the prior distribution is: ", s_prior/(r_prior)^2) -->
<!-- ``` -->

<!-- In this case we see that the variance of the prior parameters is strictly high, so this mean that our uncertainty is large at the moment. -->

$$~$$

### Conjugate Bayesian analysis

$$~$$

Initially, we need to fix the values of the prior parameters (rate and shape) of the gamma distribution:

$$
\begin{cases}
  \mathbb{E}[\theta] = \frac{s}{r} \\
  (\mathbb{Var}[\theta])^2 = \frac{s}{r^2}
\end{cases}
\rightarrow
\begin{cases}
  s = \mathbb{E}[\theta] \cdot r \\
  (\mathbb{Var}[\theta])^2 = \frac{\mathbb{E}[\theta] \cdot r}{r^2}
\end{cases}
\rightarrow
\begin{cases}
  s = \mathbb{E}[\theta] \cdot \frac{\mathbb{E}[\theta]}{(\mathbb{Var}[\theta])^2} \\
  r = \frac{\mathbb{E}[\theta]}{(\mathbb{Var}[\theta])^2}
\end{cases}
\rightarrow
\begin{cases}
  s \approx 3.007 \\
  r \approx 1002.37
\end{cases}
$$

At the end, the parameters for the prior distribution are $\pi(\theta) \sim Gamma \Big(r = 1002.37, s = 3.007 \Big)$

Now, we can setup the framework to fit it to the Bayesian analysis and get the posterior parameters, this means:

$$
\pi(\theta|y_1, ..., y_n) = \frac{\pi(y_1, ..., y_n|\theta)\pi(\theta)}{\pi(y_1, ..., y_n)} \propto \pi(y_1, ..., y_n|\theta)\pi(\theta) = \\
= \theta^{n}e^{-\theta \sum_{i=1}^{n} y_i} \cdot \theta^{s-1} e^{-r\theta} = \theta^{(s+n)-1}e^{-\theta(r+\sum_{i=1}^{n} y_i)}
$$

The posterior parameters of the gamma posterior distribution are:

$$
\pi(\theta|y_1, ..., y_n) \sim Gamma\Big(r^* = r + \sum_{i=1}^{n} y_i, \,\, s^* = s + n\Big)
$$

The shapes of the prior and the posterior follow this behaviour, considering the formula to calculate the rate and shape prior parameters and counting the mean and standard deviation considered in the previous section:

```{r fig.align="center", echo=FALSE}
E_x <- 0.003
Var_x <- (0.00173)*(0.00173)

r_prior <- E_x/Var_x
s_prior <- E_x*r_prior

curve(dgamma(x, rate=r_prior, shape=s_prior), from=0, to=0.010, xlab=expression(theta), ylab= TeX("$\\pi(\\theta) + \\pi(\\theta | y_1, ..., y_n)$"), main="", cex.main=0.5,col="blue2", ylim= c(0, 900))
grid()

r_post <- r_prior + sum(y_obs_bulbs)
s_post <- s_prior + length(y_obs_bulbs)
  
curve(dgamma(x, rate=r_post, shape=s_post), from=0, to=0.010, xlab=expression(theta), main="", cex.main=0.5,col="red2", add=TRUE, ylim= c(0, 900))
grid()

title(main = "Prior and Posterior distros", col.main= "black")
legend("topright", legend=c("Prior", "Posterior"), col=c("blue2", "red2"), lty=1, cex=0.8)
```

$$~$$

## Commenting on the characteristics of the lifetime of these innovative bulbs

$$~$$

To be more sure about the lifetime of a bulb, we can make suitable analysis considering the point estimates to see well which is the hypothetical $\theta$ parameter and later makes also the HPD interval to see where the theta parameter can fall.

We see different point estimates used in the first part of the homework:

$$~$$

- **posterior mode** $\hat{\theta}_{bayes}^{mode}$: this represents the higher value of $\theta$ for which we want: $\hat{\theta}_{bayes}^{mo} = \operatorname{argmax}_{\theta \in \Theta} \pi(\theta|y^{obs}) = \frac{s^*-1}{r^*}$

```{r fig.align="center", echo=FALSE}

print(paste("The posterior mode is equal to: ", round((s_post-1)/r_post, 4), sep=""))
```

$$~$$

- **posterior median** $\hat{\theta}_{bayes}^{med}$: this represents the $\theta$ value which divides the distribution in half for which: \ $$\hat{\theta}_{bayes}^{med} = {\displaystyle \int_{-\infty}^{\hat{\theta}_{bayes}^{med}}} \pi(\theta|y^{obs}) \,d\theta = {\displaystyle \int_{\hat{\theta}_{bayes}^{med}}}^{+\infty} \pi(\theta|y^{obs}) \,d\theta = \frac{1}{2}$$

```{r fig.align="center", echo=FALSE}
print(paste("The posterior median is equal to: ", round(qgamma(0.5, shape=s_post, rate=r_post), 4), sep=""))
```

$$~$$

- **posterior mean** $\hat{\theta}_{bayes}^{mean}$: this represents: $\hat{\theta}_{bayes}^{mean} =   \mathbb{E}(\theta|y^{obs}) = \frac{s^*}{r^*}$

```{r fig.align="center", echo=FALSE}
print(paste("The posterior mean is equal to: ", round(s_post/r_post, 4), sep=""))
```

The posterior mean is the expected value of the parameter estimate, using the posterior distribution.

$$~$$

Here we can see the point estimates over the posterior uncertainty:

```{r fig.align="center", echo=FALSE}
curve(dgamma(x, rate=r_post, shape=s_post), from=0, to=0.005, xlab=expression(theta), ylab= TeX("$\\pi(\\theta | y_1, ..., y_n)$"), main="", cex.main=0.5,col="red2")
abline(v=0.0021, lty=1, col = "green")
abline(v=0.0022,lty=1, col = "purple")
legend("topright", legend=c("Mode-Median", "Mean"), col=c("green", "purple"), lty=1, cex=0.8)
grid()

interval_estimate_hpd <- TeachingDemos::hpd(posterior.icdf = posterior_qf, conf=0.95)

print("The actual region of the HPD region is: ")
range(interval_estimate_hpd)

title(main = "Points estimates and HPD Region", col.main= "black")
abline(v=interval_estimate_hpd[1],lty=3)
abline(v=interval_estimate_hpd[2],lty=3)
abline(h=dgamma(interval_estimate_hpd[1], shape = s_post, rate = r_post), lty=3)
grid()
```

As we can see the actual $\theta$ is probably equal to 0.002, the results shown by the point estimates are similar this is due to the symmetric posterior distribution that we have.

Instead, considering the reparametrization of $\theta$ in $\psi = \frac{1}{\theta}$ we maybe say something interesting, let's do it:

- **posterior mode** $\hat{\psi}_{bayes}^{mode} = \frac{1}{\hat{\theta}_{bayes}^{mode}}$

```{r fig.align="center", echo=FALSE}
print(paste("The posterior mode is equal to: ", round(1/theta_mode, 4), sep=""))
```

- **posterior median** $\hat{\psi}_{bayes}^{med} = \frac{1}{\hat{\theta}_{bayes}^{med}}$

```{r fig.align="center", echo=FALSE}
print(paste("The posterior median is equal to: ", round(1/theta_median, 4), sep=""))
```

- **posterior mean** $\hat{\psi}_{bayes}^{mean} = \frac{1}{\hat{\theta}_{bayes}^{mean}}$

```{r fig.align="center", echo=FALSE}
print(paste("The posterior mean is equal to: ", round(1/theta_mean, 4), sep=""))
```

$$~$$

With these amazing results, the procedures in this moment are very loyal. The $\theta$ parameter is correct much probably, also because we have the posterior mean is almost equal to the true mean of these observations!

$$~$$

## Answering to Boss' request

$$~$$

To answer to Boss' request is neeeded to setup a simple procedure in order to get the probability that the average bulb lifetime $\psi = \frac{1}{\theta}$, so: 

$$
\pi(\psi > 550) = \pi\Big(\frac{1}{\theta} > 550\Big) = \pi\Big(\theta > \frac{1}{550}\Big) = 1 - \pi\Big(\theta < \frac{1}{550}\Big)
$$

Now, I can consider the theta parameters to handle this kind of probability, so the probability to have the bulb lifetime over 550 is:

$$~$$

```{r}
pgamma(1/550, shape = s_post, rate = r_post)
```

$$~$$

The probability achieved from the previous calculus says that only 22% of bulbs have a lifetime that exceeds over 550 hours. With these calculus is simple to note that considering the bayesian parameters calculated with the conjugate analysis the result achieved is more loyal, given the posterior parameters. This gives a probabily precise on the uncertainty of the parameter calculated.